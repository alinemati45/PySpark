{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book projet\n",
    "\n",
    "1- Read—Read the input data (we’re assuming a plain text file).\n",
    "\n",
    "2- Token—Tokenize each word.\n",
    "\n",
    "3- Clean—Remove any punctuation and/or tokens that aren’t words. Lowercase each word.\n",
    "\n",
    "4- Count—Count the frequency of each word present in the text.\n",
    "\n",
    "5- Answer—Return the top 10 (or 20, 50, 100).\n",
    "\n",
    "![ A simplified flow of our program, illustrating the five steps](./data/i/02-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read—Read the input data (we’re assuming a plain text file).\n",
    "### Read file and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession                                   \n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder                                                      \n",
    "         .appName(\"Analyzing the vocabulary of Pride and Prejudice.\") .getOrCreate())\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sqlContext = spark\n",
    "book = spark.read.text(\"./data/gutenberg_books/1342-0.txt\")\n",
    " \n",
    "book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ A simplified flow of our program, illustrating the five steps](./data/i/02-02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "[('value', 'string')]\n"
     ]
    }
   ],
   "source": [
    "book.printSchema()\n",
    "print(book.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The show() method takes three optional parameters:\n",
    "\n",
    "- n can be set to any positive integer and will display that number of rows.\n",
    "\n",
    "- truncate, if set to true, will truncate the columns to display only 20 characters. Set to False, it will display the whole length, or any positive integer to truncate to a specific number of characters.\n",
    "\n",
    "- vertical takes a Boolean value and, when set to True, will display each record as a small table. If you need to check records in detail, this is a very useful option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "|almost no restric...|\n",
      "|re-use it under t...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                             value|\n",
      "+--------------------------------------------------+\n",
      "|The Project Gutenberg EBook of Pride and Prejud...|\n",
      "|                                                  |\n",
      "|This eBook is for the use of anyone anywhere at...|\n",
      "|almost no restrictions whatsoever.  You may cop...|\n",
      "|re-use it under the terms of the Project Gutenb...|\n",
      "|    with this eBook or online at www.gutenberg.org|\n",
      "|                                                  |\n",
      "|                                                  |\n",
      "|                        Title: Pride and Prejudice|\n",
      "|                                                  |\n",
      "+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.show(10, truncate=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token—Tokenize each word.\n",
    "### Simple column transformations: Moving from a sentence to a list of words\n",
    "When ingesting our selected text into a data frame, PySpark created one record for each line of text and provided a value column of type String. To tokenize each word, we need to split each string into a list of distinct words. This section covers simple transformations using select(). We will split our lines of text into words so we can count them.\n",
    "\n",
    "\n",
    "- The select() method and its canonical usage, which is selecting data\n",
    "\n",
    "- The alias() method to rename transformed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                line|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|                  []|\n",
      "|[This, eBook, is,...|\n",
      "|[almost, no, rest...|\n",
      "|[re-use, it, unde...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    " \n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    " \n",
    "lines.show(5)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting specific columns using select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book.select(book.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    " \n",
    "book.select(book.value)\n",
    "book.select(book[\"value\"])\n",
    "book.select(col(\"value\"))\n",
    "book.select(\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming columns: Splitting a string into a list of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- split(value,  , -1): array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+\n",
      "| split(value,  , -1)|\n",
      "+--------------------+\n",
      "|[The, Project, Gu...|\n",
      "|                  []|\n",
      "|[This, eBook, is,...|\n",
      "|[almost, no, rest...|\n",
      "|[re-use, it, unde...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    " \n",
    "lines = book.select(split(col(\"value\"), \" \"))\n",
    " \n",
    "lines\n",
    " \n",
    " \n",
    "lines.printSchema()\n",
    " \n",
    "\n",
    "lines.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming columns: alias and withColumnRenamed\n",
    "\n",
    "- ❶ Our new column is called split(value, , -1), which isn’t really pretty.\n",
    "\n",
    "- ❷ We aliased our column to the name line. Much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- split(value,  , -1): array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- line: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.select(split(col(\"value\"), \" \")).printSchema()\n",
    "# root\n",
    "#  |-- split(value,  , -1): array (nullable = true)    ❶\n",
    "#  |    |-- element: string (containsNull = true)\n",
    " \n",
    "book.select(split(col(\"value\"), \" \").alias(\"line\")).printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When writing your code, choosing between those two options is pretty easy:\n",
    "\n",
    "- When you’re using a method where you’re specifying which columns you want to appear, like the select() method, use alias().\n",
    "\n",
    "- If you just want to rename a column without changing the rest of the data frame, use .withColumnRenamed. Note that, should the column not exist, PySpark will treat this method as a no-op and not perform anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks a lot cleaner\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "# This is messier, and you have to remember the name PySpark assigns automatically\n",
    "lines = book.select(split(book.value, \" \"))\n",
    "lines = lines.withColumnRenamed(\"split(value,  , -1)\", \"line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean—Remove any punctuation and/or tokens that aren’t words. Lowercase each word.\n",
    "\n",
    "### Reshaping your data: Exploding a list into rows\n",
    "\n",
    "![ Exploding a data frame of array[String] into a data frame of String. Each element of each array becomes its own record.](./data/i/02-04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     EBook|\n",
      "|        of|\n",
      "|     Pride|\n",
      "|       and|\n",
      "|Prejudice,|\n",
      "|        by|\n",
      "|      Jane|\n",
      "|    Austen|\n",
      "|          |\n",
      "|      This|\n",
      "|     eBook|\n",
      "|        is|\n",
      "+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    " \n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    " \n",
    "words.show(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with words: Changing case and removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|word_lower|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|     ebook|\n",
      "|        of|\n",
      "|     pride|\n",
      "|       and|\n",
      "|prejudice,|\n",
      "|        by|\n",
      "|      jane|\n",
      "|    austen|\n",
      "|          |\n",
      "|      this|\n",
      "|     ebook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
    " \n",
    "words_lower.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "|      and|\n",
      "|prejudice|\n",
      "|       by|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|         |\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word_lower\"), \"[a-z]+\", 0).alias(\"word\") \n",
    ")\n",
    " \n",
    "words_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering rows\n",
    "An important data manipulation operation is filtering records according to a certain predicate. In our case, blank cells shouldn’t be considered—they’re not words! This section covers how to filter records from a data frame. After select()-ing records, filtering is probably the most frequent and easiest operation to perform on your data; PySpark provides a simple process to do so.\n",
    "\n",
    "Conceptually, we should be able to provide a test to perform on each record. If it returns true, we keep the record. False? You’re out! PySpark provides not one, but two identical methods to perform this task. You can use either .filter() or its alias .where(). This duplication is to ease the transition for users coming from other data-processing engines or libraries; some use one, some the other. PySpark provides both, so no arguments are possible! I prefer filter(), because w maps to more data frame methods (withColumn() in chapter 4 or withColumnRenamed() in chapter 3). If we look at the next listing, we can see that columns can be compared to values using the usual Python comparison operators. In this case, we’re using “not equal,” or !=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "|      and|\n",
      "|prejudice|\n",
      "|       by|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_nonull = words_clean.filter(col(\"word\") != \"\")\n",
    " \n",
    "words_nonull.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pyspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e16f15057b3d4aa43b2e9d0470701c81ab6d7c693cdc4ec27ee425bc62e7a7a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
