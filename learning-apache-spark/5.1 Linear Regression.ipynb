{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link](https://runawayhorse001.github.io/LearningApacheSpark/regression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up spark context and SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from pyspark import SparkContext , SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sparkContext = sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|   TV|Radio|Newspaper|Sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3|  9.3|\n",
      "|151.5| 41.3|     58.5| 18.5|\n",
      "|180.8| 10.8|     58.4| 12.9|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|summary|               TV|             Radio|         Newspaper|             Sales|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|  count|              200|               200|               200|               200|\n",
      "|   mean|         147.0425|23.264000000000024|30.553999999999995|14.022500000000003|\n",
      "| stddev|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|\n",
      "|    min|              0.7|               0.0|               0.3|               1.6|\n",
      "|    max|            296.4|              49.6|             114.0|              27.0|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv').\\\n",
    "                       options(header='true', \\\n",
    "                       inferschema='true').\\\n",
    "            load(\"../data/Advertising.csv\",header=True);\n",
    "df.show(5,True)\n",
    "df.printSchema()\n",
    "df.describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Convert the data to dense vector (features and label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# I provide two ways to build the features and labels\n",
    "\n",
    "# method 1 (good for small feature):\n",
    "#def transData(row):\n",
    "#    return Row(label=row[\"Sales\"],\n",
    "#               features=Vectors.dense([row[\"TV\"],\n",
    "#                                       row[\"Radio\"],\n",
    "#                                       row[\"Newspaper\"]]))\n",
    "\n",
    "# Method 2 (good for large features):\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Transform the dataset to DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[230.1,37.8,69.2]| 22.1|\n",
      "| [44.5,39.3,45.1]| 10.4|\n",
      "| [17.2,45.9,69.3]|  9.3|\n",
      "|[151.5,41.3,58.5]| 18.5|\n",
      "|[180.8,10.8,58.4]| 12.9|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed= transData(df)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Deal With Categorical Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----------------+\n",
      "|         features|label|  indexedFeatures|\n",
      "+-----------------+-----+-----------------+\n",
      "|[230.1,37.8,69.2]| 22.1|[230.1,37.8,69.2]|\n",
      "| [44.5,39.3,45.1]| 10.4| [44.5,39.3,45.1]|\n",
      "| [17.2,45.9,69.3]|  9.3| [17.2,45.9,69.3]|\n",
      "|[151.5,41.3,58.5]| 18.5|[151.5,41.3,58.5]|\n",
      "|[180.8,10.8,58.4]| 12.9|[180.8,10.8,58.4]|\n",
      "+-----------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \\\n",
    "                               outputCol=\"indexedFeatures\",\\\n",
    "                               maxCategories=4).fit(transformed)\n",
    "\n",
    "data = featureIndexer.transform(transformed)\n",
    "data.show(5,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into training and test sets (40% held out for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|       features|label|\n",
      "+---------------+-----+\n",
      "| [5.4,29.9,9.4]|  5.3|\n",
      "|[7.3,28.1,41.4]|  5.5|\n",
      "|[7.8,38.9,50.6]|  6.6|\n",
      "| [8.4,27.2,2.1]|  5.7|\n",
      "|  [8.6,2.1,1.0]|  4.8|\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+-----+\n",
      "|        features|label|\n",
      "+----------------+-----+\n",
      "|  [0.7,39.6,8.7]|  1.6|\n",
      "|  [4.1,11.6,5.7]|  3.2|\n",
      "| [8.7,48.9,75.0]|  7.2|\n",
      "|[11.7,36.9,45.2]|  7.3|\n",
      "| [13.1,0.4,25.6]|  5.3|\n",
      "+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (40% held out for testing)\n",
    "(trainingData, testData) = transformed.randomSplit([0.6, 0.4])\n",
    "trainingData.show(5)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression class\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "# Define LinearRegression algorithm\n",
    "lr = LinearRegression()\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, lr])\n",
    "model = pipeline.fit(trainingData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of the Model\n",
    "Spark has a poor summary function for data and model. I wrote a summary function which has similar format as R output for the linear regression in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: the last rows are the information for Intercept\n",
      "## -------------------------------------------------\n",
      "##   Estimate   |   Std.Error | t Values  |  P-value\n",
      "##   0.045114   0.001540   29.299   0.000000\n",
      "##   0.196999   0.009781   20.141   0.000000\n",
      "##  -0.002401   0.006222   -0.386   0.700238\n",
      "##   2.914105   0.351133    8.299   0.000000\n",
      "## ---\n",
      "## Mean squared error:  2.232753 , RMSE:  1.494240\n",
      "## Multiple R-squared: 0.917570 ,             Total iterations: 1\n"
     ]
    }
   ],
   "source": [
    "def modelsummary(model):\n",
    "    import numpy as np\n",
    "    print (\"Note: the last rows are the information for Intercept\")\n",
    "    print (\"##\",\"-------------------------------------------------\")\n",
    "    print (\"##\",\"  Estimate   |   Std.Error | t Values  |  P-value\")\n",
    "    coef = np.append(list(model.coefficients),model.intercept)\n",
    "    Summary=model.summary\n",
    "\n",
    "    for i in range(len(Summary.pValues)):\n",
    "        print (\"##\",'{:10.6f}'.format(coef[i]),\\\n",
    "        '{:10.6f}'.format(Summary.coefficientStandardErrors[i]),\\\n",
    "        '{:8.3f}'.format(Summary.tValues[i]),\\\n",
    "        '{:10.6f}'.format(Summary.pValues[i]))\n",
    "\n",
    "    print (\"##\",'---')\n",
    "    print (\"##\",\"Mean squared error: % .6f\" \\\n",
    "           % Summary.meanSquaredError, \", RMSE: % .6f\" \\\n",
    "           % Summary.rootMeanSquaredError )\n",
    "    print (\"##\",\"Multiple R-squared: %f\" % Summary.r2, \", \\\n",
    "            Total iterations: %i\"% Summary.totalIterations)\n",
    "modelsummary(model.stages[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: double, indexedFeatures: vector, prediction: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+----------------+\n",
      "|        features|label| indexedFeatures|\n",
      "+----------------+-----+----------------+\n",
      "|  [0.7,39.6,8.7]|  1.6|  [0.7,39.6,8.7]|\n",
      "|  [4.1,11.6,5.7]|  3.2|  [4.1,11.6,5.7]|\n",
      "| [8.7,48.9,75.0]|  7.2| [8.7,48.9,75.0]|\n",
      "|[11.7,36.9,45.2]|  7.3|[11.7,36.9,45.2]|\n",
      "| [13.1,0.4,25.6]|  5.3| [13.1,0.4,25.6]|\n",
      "|[17.2,45.9,69.3]|  9.3|[17.2,45.9,69.3]|\n",
      "|[19.6,20.1,17.0]|  7.6|[19.6,20.1,17.0]|\n",
      "| [25.6,39.0,9.3]|  9.5| [25.6,39.0,9.3]|\n",
      "|[36.9,38.6,65.6]| 10.8|[36.9,38.6,65.6]|\n",
      "| [39.5,41.1,5.8]| 10.8| [39.5,41.1,5.8]|\n",
      "|[43.1,26.7,35.1]| 10.1|[43.1,26.7,35.1]|\n",
      "|[44.7,25.8,20.6]| 10.1|[44.7,25.8,20.6]|\n",
      "| [53.5,2.0,21.4]|  8.1| [53.5,2.0,21.4]|\n",
      "| [56.2,5.7,29.7]|  8.7| [56.2,5.7,29.7]|\n",
      "|[57.5,32.8,23.5]| 11.8|[57.5,32.8,23.5]|\n",
      "|[59.6,12.0,43.1]|  9.7|[59.6,12.0,43.1]|\n",
      "|[62.3,12.6,18.3]|  9.7|[62.3,12.6,18.3]|\n",
      "| [66.1,5.8,24.2]|  8.6| [66.1,5.8,24.2]|\n",
      "|[70.6,16.0,40.8]| 10.5|[70.6,16.0,40.8]|\n",
      "|[75.3,20.3,32.5]| 11.3|[75.3,20.3,32.5]|\n",
      "+----------------+-----+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"features\",\"label\",\"indexedFeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 1.97036\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score: 0.8525568836200761\n"
     ]
    }
   ],
   "source": [
    "y_true = predictions.select(\"label\").toPandas()\n",
    "y_pred = predictions.select(\"prediction\").toPandas()\n",
    "\n",
    "import sklearn.metrics\n",
    "r2_score = sklearn.metrics.r2_score(y_true, y_pred)\n",
    "print('r2_score: {0}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
