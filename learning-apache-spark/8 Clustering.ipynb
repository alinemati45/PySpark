{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "Set up spark context and SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def mapLibSVM(row): \n",
    "    return (row[5],Vectors.dense(row[:3]))\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark K-means example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal.length|sepal.width|petal.length|petal.width|variety|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| Setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| Setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| Setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| Setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| Setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- sepal.length: double (nullable = true)\n",
      " |-- sepal.width: double (nullable = true)\n",
      " |-- petal.length: double (nullable = true)\n",
      " |-- petal.width: double (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      "\n",
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "|summary|      sepal.length|        sepal.width|      petal.length|       petal.width|  variety|\n",
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "|  count|               150|                150|               150|               150|      150|\n",
      "|   mean| 5.843333333333335|  3.057333333333334|3.7580000000000027| 1.199333333333334|     null|\n",
      "| stddev|0.8280661279778637|0.43586628493669793|1.7652982332594662|0.7622376689603467|     null|\n",
      "|    min|               4.3|                2.0|               1.0|               0.1|   Setosa|\n",
      "|    max|               7.9|                4.4|               6.9|               2.5|Virginica|\n",
      "+-------+------------------+-------------------+------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = spark.read.format('com.databricks.spark.csv').\\\n",
    "                       options(header='true', \\\n",
    "                       inferschema='true').\\\n",
    "            load(\"./data/iris.csv\",header=True);\n",
    "# check the data set\n",
    "\n",
    "df.show(5,True)\n",
    "df.printSchema()\n",
    "\n",
    "df.describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the data to dense vector (features)\n",
    "\n",
    "You are strongly encouraged to try my get_dummy function for dealing with the categorical data in complex dataset.\n",
    "\n",
    "## Supervised learning version:\n",
    "```python\n",
    "def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol):\n",
    "\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "                 for c in categoricalCols ]\n",
    "\n",
    "    # default setting: dropLast=True\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "                 outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n",
    "                 for indexer in indexers ]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\n",
    "                                + continuousCols, outputCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "\n",
    "    data = data.withColumn('label',col(labelCol))\n",
    "\n",
    "    return data.select(indexCol,'features','label')\n",
    "```\n",
    "## Unsupervised learning version:\n",
    "```python\n",
    "\n",
    "def get_dummy(df,indexCol,categoricalCols,continuousCols):\n",
    "    '''\n",
    "    Get dummy variables and concat with continuous variables for unsupervised learning.\n",
    "    :param df: the dataframe\n",
    "    :param categoricalCols: the name list of the categorical data\n",
    "    :param continuousCols:  the name list of the numerical data\n",
    "    :return k: feature matrix\n",
    "\n",
    "    :author: Wenqiang Feng\n",
    "    :email:  von198@gmail.com\n",
    "    '''\n",
    "\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "                 for c in categoricalCols ]\n",
    "\n",
    "    # default setting: dropLast=True\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "                 outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n",
    "                 for indexer in indexers ]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\n",
    "                                + continuousCols, outputCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "\n",
    "    return data.select(indexCol,'features')\n",
    "```\n",
    "## Two in one:\n",
    "```python\n",
    "def get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol,dropLast=False):\n",
    "\n",
    "    '''\n",
    "    Get dummy variables and concat with continuous variables for ml modeling.\n",
    "    :param df: the dataframe\n",
    "    :param categoricalCols: the name list of the categorical data\n",
    "    :param continuousCols:  the name list of the numerical data\n",
    "    :param labelCol:  the name of label column\n",
    "    :param dropLast:  the flag of drop last column\n",
    "    :return: feature matrix\n",
    "\n",
    "    :author: Wenqiang Feng\n",
    "    :email:  von198@gmail.com\n",
    "\n",
    "    >>> df = spark.createDataFrame([\n",
    "                  (0, \"a\"),\n",
    "                  (1, \"b\"),\n",
    "                  (2, \"c\"),\n",
    "                  (3, \"a\"),\n",
    "                  (4, \"a\"),\n",
    "                  (5, \"c\")\n",
    "              ], [\"id\", \"category\"])\n",
    "\n",
    "    >>> indexCol = 'id'\n",
    "    >>> categoricalCols = ['category']\n",
    "    >>> continuousCols = []\n",
    "    >>> labelCol = []\n",
    "\n",
    "    >>> mat = get_dummy(df,indexCol,categoricalCols,continuousCols,labelCol)\n",
    "    >>> mat.show()\n",
    "\n",
    "    >>>\n",
    "        +---+-------------+\n",
    "        | id|     features|\n",
    "        +---+-------------+\n",
    "        |  0|[1.0,0.0,0.0]|\n",
    "        |  1|[0.0,0.0,1.0]|\n",
    "        |  2|[0.0,1.0,0.0]|\n",
    "        |  3|[1.0,0.0,0.0]|\n",
    "        |  4|[1.0,0.0,0.0]|\n",
    "        |  5|[0.0,1.0,0.0]|\n",
    "        +---+-------------+\n",
    "    '''\n",
    "\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "                 for c in categoricalCols ]\n",
    "\n",
    "    # default setting: dropLast=True\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "                 outputCol=\"{0}_encoded\".format(indexer.getOutputCol()),dropLast=dropLast)\n",
    "                 for indexer in indexers ]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\n",
    "                                + continuousCols, outputCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "\n",
    "    if indexCol and labelCol:\n",
    "        # for supervised learning\n",
    "        data = data.withColumn('label',col(labelCol))\n",
    "        return data.select(indexCol,'features','label')\n",
    "    elif not indexCol and labelCol:\n",
    "        # for supervised learning\n",
    "        data = data.withColumn('label',col(labelCol))\n",
    "        return data.select('features','label')\n",
    "    elif indexCol and not labelCol:\n",
    "        # for unsupervised learning\n",
    "        return data.select(indexCol,'features')\n",
    "    elif not indexCol and not labelCol:\n",
    "        # for unsupervised learning\n",
    "        return data.select('features')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal.length', 'sepal.width', 'petal.length', 'petal.width', 'variety']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+----------+\n",
      "|sepal.length|sepal.width|petal.length|petal.width|variety|labelIndex|\n",
      "+------------+-----------+------------+-----------+-------+----------+\n",
      "|         5.1|        3.5|         1.4|        0.2| Setosa|       0.0|\n",
      "|         4.9|        3.0|         1.4|        0.2| Setosa|       0.0|\n",
      "|         4.7|        3.2|         1.3|        0.2| Setosa|       0.0|\n",
      "|         4.6|        3.1|         1.5|        0.2| Setosa|       0.0|\n",
      "|         5.0|        3.6|         1.4|        0.2| Setosa|       0.0|\n",
      "|         5.4|        3.9|         1.7|        0.4| Setosa|       0.0|\n",
      "|         4.6|        3.4|         1.4|        0.3| Setosa|       0.0|\n",
      "|         5.0|        3.4|         1.5|        0.2| Setosa|       0.0|\n",
      "|         4.4|        2.9|         1.4|        0.2| Setosa|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1| Setosa|       0.0|\n",
      "|         5.4|        3.7|         1.5|        0.2| Setosa|       0.0|\n",
      "|         4.8|        3.4|         1.6|        0.2| Setosa|       0.0|\n",
      "|         4.8|        3.0|         1.4|        0.1| Setosa|       0.0|\n",
      "|         4.3|        3.0|         1.1|        0.1| Setosa|       0.0|\n",
      "|         5.8|        4.0|         1.2|        0.2| Setosa|       0.0|\n",
      "|         5.7|        4.4|         1.5|        0.4| Setosa|       0.0|\n",
      "|         5.4|        3.9|         1.3|        0.4| Setosa|       0.0|\n",
      "|         5.1|        3.5|         1.4|        0.3| Setosa|       0.0|\n",
      "|         5.7|        3.8|         1.7|        0.3| Setosa|       0.0|\n",
      "|         5.1|        3.8|         1.5|        0.3| Setosa|       0.0|\n",
      "+------------+-----------+------------+-----------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-------------+\n",
      "|variety|     features|\n",
      "+-------+-------------+\n",
      "|    0.0|[5.1,3.5,1.4]|\n",
      "|    0.0|[4.9,3.0,1.4]|\n",
      "|    0.0|[4.7,3.2,1.3]|\n",
      "|    0.0|[4.6,3.1,1.5]|\n",
      "|    0.0|[5.0,3.6,1.4]|\n",
      "|    0.0|[5.4,3.9,1.7]|\n",
      "|    0.0|[4.6,3.4,1.4]|\n",
      "|    0.0|[5.0,3.4,1.5]|\n",
      "|    0.0|[4.4,2.9,1.4]|\n",
      "|    0.0|[4.9,3.1,1.5]|\n",
      "|    0.0|[5.4,3.7,1.5]|\n",
      "|    0.0|[4.8,3.4,1.6]|\n",
      "|    0.0|[4.8,3.0,1.4]|\n",
      "|    0.0|[4.3,3.0,1.1]|\n",
      "|    0.0|[5.8,4.0,1.2]|\n",
      "|    0.0|[5.7,4.4,1.5]|\n",
      "|    0.0|[5.4,3.9,1.3]|\n",
      "|    0.0|[5.1,3.5,1.4]|\n",
      "|    0.0|[5.7,3.8,1.7]|\n",
      "|    0.0|[5.1,3.8,1.5]|\n",
      "+-------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"variety\", outputCol=\"labelIndex\")\n",
    "indexer = indexer.fit(df).transform(df)\n",
    "indexer.show()\n",
    "df = indexer.rdd.map(mapLibSVM).toDF([\"variety\", \"features\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a k-means model (Estimator).\n",
    "kmeans = KMeans().setK(3).setSeed(3)\n",
    "model = kmeans.fit(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.7353747891731895\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "predictions = model.transform(df)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7353747891731895"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[5.86833333 2.74       4.38166667]\n",
      "[5.006 3.428 1.462]\n",
      "[6.8525 3.07   5.6925]\n"
     ]
    }
   ],
   "source": [
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|prediction|variety|\n",
      "+----------+-------+\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "|         1|    0.0|\n",
      "+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = predictions.select([\"prediction\",\"variety\"])\n",
    "predictions.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pyspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e16f15057b3d4aa43b2e9d0470701c81ab6d7c693cdc4ec27ee425bc62e7a7a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
